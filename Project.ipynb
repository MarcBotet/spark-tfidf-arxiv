{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637d547f",
   "metadata": {},
   "source": [
    "We assume that a cassandra database has been already created and filled up with papers. If not please check fetch_papers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a548c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuratins related to Cassandra connector, graphframes & Cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.0,graphframes:graphframes:0.8.2-spark2.4-s_2.11 --conf spark.cassandra.connection.host=127.0.0.1 --repositories https://repos.spark-packages.org/ pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4dad679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d03f8",
   "metadata": {},
   "source": [
    "## LOAD THE DATA FROM CASSANDRA IN A RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611aeab8",
   "metadata": {},
   "source": [
    "for tf-idf we only will use 2.5k to overcome computational issues on a laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d63f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /home/mbc96/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mbc96/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/mnt/c/Users/markb/Documents/0kth/data_intensive/spark-2.4.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e5b68fa2-9761-4efe-8dc5-8a2eb9973433;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.11;2.4.0 in central\n",
      "\tfound com.twitter#jsr166e;1.1.0 in central\n",
      "\tfound io.netty#netty-all;4.0.33.Final in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.3 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound joda-time#joda-time;2.3 in central\n",
      "\tfound org.joda#joda-convert;1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in central\n",
      "\tfound graphframes#graphframes;0.8.2-spark2.4-s_2.11 in spark-list\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 1623ms :: artifacts dl 29ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.11;2.4.0 from central in [default]\n",
      "\tcom.twitter#jsr166e;1.1.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.3 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.2-spark2.4-s_2.11 from spark-list in [default]\n",
      "\tio.netty#netty-all;4.0.33.Final from central in [default]\n",
      "\tjoda-time#joda-time;2.3 from central in [default]\n",
      "\torg.joda#joda-convert;1.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e5b68fa2-9761-4efe-8dc5-8a2eb9973433\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/27ms)\n",
      "21/10/24 19:23:43 WARN Utils: Your hostname, DESKTOP-2R0KHC8 resolves to a loopback address: 127.0.1.1; using 172.27.226.188 instead (on interface eth0)\n",
      "21/10/24 19:23:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/10/24 19:23:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Creating PySpark Context\n",
    "sc = SparkContext(\"local\", \"arXiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490e9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e70f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"papers\", keyspace=\"papers_space\").load().limit(2500).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fda2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "number_docs = rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f66b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def process(pid, title, abstract):\n",
    "    text = title + ' ' + abstract\n",
    "    doc = word_tokenize(strip_punctuation(text).lower())\n",
    "    doc = [w for w in doc if w not in stop_words]\n",
    "    return [((pid, w), 1) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b2b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = rdd.flatMap(lambda x: process(x.pid, x.title, x.abstract))\n",
    "tf = map1.reduceByKey(lambda x,y: x + y) # number of times that a word appear in a given document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d540c",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2d6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.map(lambda x: (x[0][1], 1)) \n",
    "frequency = words.reduceByKey(lambda x,y: x + y) # num of docs that contains the word\n",
    "# let's remove the number of words smaller than 2\n",
    "frequency = frequency.filter(lambda x: x[1] >= 2)\n",
    "freq = frequency.map(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a72244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of uniques words\n",
    "freq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ede474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = frequency.map(lambda x: (x[0], np.log(number_docs/x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68a151",
   "metadata": {},
   "source": [
    "## tf-idf vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8259c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tf = tf.map(lambda x: (x[0][1], (x[0][0], x[1]))) # we need only the word as a key to easily join the tf with idf\n",
    "\n",
    "tfidf_word = re_tf.join(idf)\n",
    "tfidf = tfidf_word.map(lambda x: (x[1][0][0], (x[0], x[1][0][1]*x[1][1])))\n",
    "\n",
    "l2norm = tfidf.map(lambda x: (x[0], x[1][1]))\n",
    "l2norm = l2norm.reduceByKey(lambda x, y: np.sqrt(x**2+y**2))\n",
    "\n",
    "# the tf-idf value for each word normalized\n",
    "tfidf_norm = tfidf.join(l2norm)\n",
    "tfidf_norm = tfidf_norm.map(lambda x: (x[0], (x[1][0][0], x[1][0][1]/x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543b6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tf-idf value for each word normalized\n",
    "tfidf_norm = tfidf.join(l2norm)\n",
    "tfidf_norm = tfidf_norm.map(lambda x: (x[0], (x[1][0][0], x[1][0][1]/x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02663122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (2 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.8 ms, sys: 20.9 ms, total: 56.8 ms\n",
      "Wall time: 20.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2107.05727', ('image', 0.04399861543747763)),\n",
       " ('2107.05727', ('step', 0.06269436493223962)),\n",
       " ('2107.05727', ('defined', 0.07329196078327006)),\n",
       " ('2107.05727', ('resulting', 0.06714407108767272)),\n",
       " ('2107.05727', ('based', 0.01712151704858696))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#this is the normalized tfidf for all the documents\n",
    "tfidf_norm.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f6c38",
   "metadata": {},
   "source": [
    "# Calculate the matrix for all the scores for each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc7eb0",
   "metadata": {},
   "source": [
    "This calcul is tedious, it requires to multiply the matrix X = D x W, X * X^T, being W the unique words and D the document set. Due to this is executed in a single laptop, as a demo purposes to try spark, it has been reduced the dimensionality of documents. In a real world scneario spark would be able to make use of multiple resources to scale the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5efb22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                         (0 + 1) / 1]\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('image', ('2107.05727', 0.04399861543747763)),\n",
       " ('step', ('2107.05727', 0.06269436493223962))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the transpose of tfidf, that is, using word as a key.\n",
    "word_key = tfidf_norm.map(lambda x: (x[1][0], (x[0], x[1][1])))\n",
    "word_key.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e52a7",
   "metadata": {},
   "source": [
    "We implement the matrix multiplication in spark, making use of MapReduce. Due to X * X^T is a symmetric matrix, we only need to make half of the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b101c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(z):\n",
    "    my_sorted_list = sorted(z[1], key=lambda x: x[1])\n",
    "    combinations = itertools.combinations(z[1], 2)\n",
    "    return [((x[0],y[0]), x[1]*y[1]) for x,y in combinations]\n",
    "    \n",
    "\n",
    "first_stage = word_key.groupByKey()\n",
    "multi = first_stage.flatMap(product)\n",
    "final = multi.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dec3cb",
   "metadata": {},
   "source": [
    "final RDD contains the similarity score for each paper pair in the corpus, having used n=2500 papers it contains a     (n * n-1)/2 pairs, which is over 3 millions pairs. Due to most of this papers are not relevant given a paper, and we are only interested in the most relevant ones, we do not need to store the full matrix. Instead we perform filtering to delete all the paper pairs which are below 0.1 similarity (assuming low similarity being no related). This filering reduces the amount of paper pairs to 28k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6624d49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017161328128935685"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.map(lambda x: x[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07aa44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 326:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.7 ms, sys: 13.4 ms, total: 46 ms\n",
      "Wall time: 28.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27976"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final_relevant = final.filter(lambda x: x[1] >= 0.1)\n",
    "final_relevant.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc4859e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.4 ms, sys: 10.2 ms, total: 85.6 ms\n",
      "Wall time: 9.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_result = final_relevant.map(lambda x: (x[0][0], x[0][1], float(x[1]))).toDF(['doc1', 'doc2', 'score'])\n",
    "df_result = df_result.orderBy(desc('score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a8d790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 336:==========================>                          (100 + 2) / 200]\r",
      "[Stage 336:====================================>                (138 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+\n",
      "|        max(score)|         min(score)|        avg(score)|\n",
      "+------------------+-------------------+------------------+\n",
      "|0.8553202975106373|0.10000189920035375|0.1490131027699097|\n",
      "+------------------+-------------------+------------------+\n",
      "\n",
      "CPU times: user 37.2 ms, sys: 33.1 ms, total: 70.3 ms\n",
      "Wall time: 2.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 336:==============================================>      (176 + 2) / 200]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_result.registerTempTable(\"df_result\")\n",
    "spark.sql(\"SELECT max(score), min(score), avg(score) FROM df_result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba269d8",
   "metadata": {},
   "source": [
    "Finally, we store the result in a cassandra table and a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc04ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:===================================================>  (190 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 112 ms, total: 310 ms\n",
      "Wall time: 1min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 83:====================================================> (195 + 2) / 200]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_result.write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"similarity_table\", keyspace=\"papers_space\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e01b8e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 300 ms, sys: 195 ms, total: 495 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_result.write.mode(\"append\").save('data_similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ce8ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+\n",
      "|      doc1|      doc2|             score|\n",
      "+----------+----------+------------------+\n",
      "|2106.09770|2109.03663|0.8553202975106373|\n",
      "|2103.02354|2101.01292|0.6431122622671059|\n",
      "|1908.06957|2110.01899|0.6427000822557831|\n",
      "|2106.04300|2004.01935| 0.638596119162532|\n",
      "|2109.13797|2105.05609|0.6080651106648617|\n",
      "+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 13.1 ms, sys: 29.9 ms, total: 43 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74efe6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|      doc1|      doc2|              score|\n",
      "+----------+----------+-------------------+\n",
      "|2106.09770|2109.03663| 0.8553202975106373|\n",
      "|2106.09770|2110.07947| 0.4145166502739236|\n",
      "|2106.09770|2107.07925|0.41123931371500433|\n",
      "|2106.09770|2110.04073| 0.3951389001824246|\n",
      "|2103.00534|2106.09770| 0.3650950311347457|\n",
      "+----------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 439 µs, sys: 11.2 ms, total: 11.7 ms\n",
      "Wall time: 189 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = '2106.09770'\n",
    "df_result.filter((f'doc1 == {doc} or doc2=={doc}')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c506a",
   "metadata": {},
   "source": [
    "Document comparision one by one. We also provide the code to calculate the tf-idf one paper at the time. (this code is unfeasible to use in the complete collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16fd9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a document d\n",
    "d = '2106.09770'\n",
    "\n",
    "rdd_d = word_key.filter(lambda x: x[1][0] == d)\n",
    "rest = word_key.filter(lambda x: x[1][0] != d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7192910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.1 ms, sys: 35.3 ms, total: 99.4 ms\n",
      "Wall time: 6.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2109.03663', 0.8553202975106372),\n",
       " ('2110.07947', 0.4145166502739237),\n",
       " ('2107.07925', 0.4112393137150044),\n",
       " ('2110.04073', 0.3951389001824246),\n",
       " ('2103.00534', 0.3650950311347456)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#having only a small RDD (only one paper), using broadcastinc is faster than a normal join\n",
    "\n",
    "def joining(k,v):\n",
    "    value = smallLookup.value.get(k)\n",
    "    value = (0, 0) if value is None else value\n",
    "    return (k, (v, value))\n",
    "\n",
    "smallLookup = sc.broadcast(rdd_d.collectAsMap())\n",
    "\n",
    "our_join = rest.map(lambda x: joining(x[0], x[1]))\n",
    "our_join = our_join.filter(lambda x: x[1][1] is not None)\n",
    "\n",
    "#our_join = rest.join(rdd_d)\n",
    "\n",
    "value_d = our_join.map(lambda x: (x[1][0][0], x[1][0][1]*x[1][1][1]))\n",
    "\n",
    "value_d.reduceByKey(lambda x, y: x+y).takeOrdered(5, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7af76",
   "metadata": {},
   "source": [
    "# Co-authorship analysis using graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03152ab2",
   "metadata": {},
   "source": [
    "We use the library for spark DataFrames, supported by Databricks, which is inspired in GraphX and, diferenly of GraphX, provides a Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from graphframes.lib import AggregateMessages as AM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b314eb",
   "metadata": {},
   "source": [
    "For graph analysis we use the complete amount of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"papers\", keyspace=\"papers_space\").load().rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c407e1",
   "metadata": {},
   "source": [
    "Due to the inability of defining undirected weighted graph, as it is a co-authorship graph, we replicate the edges as much as need it and are repeated in both direction. That gives the necessity of dividing by two the number of edges to get some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_edges = rdd.flatMap(lambda a: [x for x in itertools.product(a.authors,a.authors) if x[0] != x[1]])\n",
    "rdd_vertices = rdd.flatMap(lambda x: x.authors).distinct().map(lambda x: (x, x))\n",
    "\n",
    "edges = sqlContext.createDataFrame(rdd_edges, ['src', 'dst'])\n",
    "vertices = sqlContext.createDataFrame(rdd_vertices, ['id', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices, edges)\n",
    "vertices.cache()\n",
    "edges.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28663100",
   "metadata": {},
   "source": [
    "## Basic statistics to get insides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49874327",
   "metadata": {},
   "source": [
    "Number of unics authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed811a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106382"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183e100",
   "metadata": {},
   "source": [
    "Average of co-authors per author. That is, when an author write a paper it collaborates with a certain number of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99258cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.03733714350172"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_coauhtors = (graph.edges.count()/2) / graph.vertices.count()\n",
    "avg_coauhtors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b3f65",
   "metadata": {},
   "source": [
    "inDegree represent a collaboration between authors. Note that in inDegree it doesn't show in the stats the vertices with no edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5b247eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================================>(197 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------------+\n",
      "|max(inDegree)|min(inDegree)|     avg(inDegree)|\n",
      "+-------------+-------------+------------------+\n",
      "|          732|            1|10.221683897302865|\n",
      "+-------------+-------------+------------------+\n",
      "\n",
      "CPU times: user 90.5 ms, sys: 49.5 ms, total: 140 ms\n",
      "Wall time: 16.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph.inDegrees.registerTempTable(\"indegrees\")\n",
    "spark.sql(\"SELECT max(inDegree), avg(inDegree) FROM indegrees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7574c",
   "metadata": {},
   "source": [
    "Number of authors with any collaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff9413ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.count() - agg_new.agg({\"id\":\"count\"}).collect()[0]['count(id)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c650a72",
   "metadata": {},
   "source": [
    "Avarage number of the unique co-authors that each author has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "027bd528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.45911902389502"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_coauthors = agg_new.agg({\"unique_coauthors\":\"sum\"}).collect()[0]['sum(unique_coauthors)']\n",
    "unique_coauthors = (sum_coauthors/2) / graph.vertices.count()\n",
    "unique_coauthors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad03066",
   "metadata": {},
   "source": [
    "Average of the times an author collaborates with them co-authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eefbfdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1296709319729326"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_coauhtors / unique_coauthors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380c536",
   "metadata": {},
   "source": [
    "Pagerank calculations of the co-authors graph. PageRank shows us the most relavant auhtors based on its collaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to read pagerank from cassandra instead\n",
    "# pagerank_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"all_pagerank_table\", keyspace=\"papers_space\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ff20027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 922:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                  id|          pagerank|\n",
      "+--------------------+------------------+\n",
      "|         Alois Knoll| 5.286543865009098|\n",
      "|      Amaury Habrard|1.9593054937387528|\n",
      "|        Weiqing Wang|1.4275080607067454|\n",
      "|          Boyuan Liu|1.3161656619492115|\n",
      "|         Rasha Faqeh|0.9879530695268659|\n",
      "|    Michael Kaminski|0.8614367662782962|\n",
      "|  Alfred O. Hero III|1.0298469794062282|\n",
      "|      Yofti Milkessa|0.6539744868031192|\n",
      "|      Mattias Marder|0.8444698579560116|\n",
      "|        Sadik Bessou|1.0123761080685005|\n",
      "|        Camile Sothe|0.9565246366559708|\n",
      "|   Sanjeev J. Koppal|1.0123761080685005|\n",
      "|        Shubham Negi|0.5548129013488963|\n",
      "|    Artem Vysogorets|1.0123761080685005|\n",
      "|      Benjamin Ampel|1.0123761080685005|\n",
      "|        Nitish Kumar|0.6900420997385767|\n",
      "|Vasileios Mavroeidis|1.0123761080685005|\n",
      "|       Austen Z. Fan|1.0123761080685005|\n",
      "|     Shuvalaxmi Dass|0.6580036970356548|\n",
      "|       Masato Kimura|1.0123761080685005|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 8.21 s, sys: 3.64 s, total: 11.8 s\n",
      "Wall time: 2h 26min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "pagerank_df = graph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "pagerank_df.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ae20948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 299:============================>                            (2 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------------+\n",
      "|max(pagerank)|min(pagerank)|     avg(pagerank)|\n",
      "+-------------+-------------+------------------+\n",
      "|    37.952694|   0.15185642|0.9999999932396952|\n",
      "+-------------+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pagerank_df.registerTempTable(\"pagerank_table\")\n",
    "spark.sql(\"SELECT max(pagerank), min(pagerank), avg(pagerank) FROM pagerank_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a9705de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 301:============================>                            (2 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|       id| pagerank|\n",
      "+---------+---------+\n",
      "| Yang Liu|37.952694|\n",
      "| Wei Wang| 30.71963|\n",
      "| Xin Wang| 22.62668|\n",
      "| Hao Wang|20.407763|\n",
      "| Jun Wang|20.171339|\n",
      "|    Bo Li|20.043365|\n",
      "| Jie Zhou|19.897903|\n",
      "|Wei Zhang| 18.64152|\n",
      "|  Yang Li|17.837828|\n",
      "|Lei Zhang|17.259756|\n",
      "+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pagerank_df.orderBy(desc('pagerank')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fe811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving pageranks results\n",
    "pagerank_df.vertices.select(\"id\", \"pagerank\").write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"all_pagerank_table\", keyspace=\"papers_space\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d658f",
   "metadata": {},
   "source": [
    "## Community detection (Label propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791906a",
   "metadata": {},
   "source": [
    "Community detection algorithms aim to discover and organize the subgrups of vertices in the graph that are more frequently connectet between them. GraphFrames provides the label propagation algorithm to identify those communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3dc2a59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 465:===================================================> (194 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                  id|        label|\n",
      "+--------------------+-------------+\n",
      "|         Alois Knoll|1649267442202|\n",
      "|      Amaury Habrard| 584115552585|\n",
      "|        Weiqing Wang| 901943132343|\n",
      "|          Boyuan Liu|1176821039283|\n",
      "|         Rasha Faqeh|1649267441741|\n",
      "|    Michael Kaminski|1116691497170|\n",
      "|  Alfred O. Hero III|1176821039374|\n",
      "|      Yofti Milkessa|1228360646932|\n",
      "|      Mattias Marder|1563368096108|\n",
      "|        Sadik Bessou|1546188226720|\n",
      "|        Camile Sothe| 257698037846|\n",
      "|   Sanjeev J. Koppal|1194000908358|\n",
      "|        Shubham Negi| 188978561025|\n",
      "|    Artem Vysogorets| 188978561260|\n",
      "|      Benjamin Ampel|  68719476798|\n",
      "|        Nitish Kumar| 858993459313|\n",
      "|Vasileios Mavroeidis|  68719477201|\n",
      "|       Austen Z. Fan| 137438953705|\n",
      "|     Shuvalaxmi Dass| 884763263280|\n",
      "|       Masato Kimura| 627065225436|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 5.51 s, sys: 3.1 s, total: 8.61 s\n",
      "Wall time: 29min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "communities = graph.labelPropagation(maxIter=5)\n",
    "communities.select(\"id\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to cassandra\n",
    "communities.select(\"id\", \"label\").write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"community_table\", keyspace=\"papers_space\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee40d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to read from cassandra instead of re calculating\n",
    "communities = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"community_table\", keyspace=\"papers_space\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f581de2",
   "metadata": {},
   "source": [
    "There is a total of different 2756 communities. For example, Yang Liu, the author with highest PageRank, pertains in the 99 group wich has 159 members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "56824fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27656"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb0e8385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|      id|label|\n",
      "+--------+-----+\n",
      "|Yang Liu|   99|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "communities.filter((\"id=='Yang Liu'\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12531439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities.filter((\"label==99\"))communities.select(\"id\", \"label\").write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"community_table\", keyspace=\"papers_space\").save().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d040d69a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                  id|label|\n",
      "+--------------------+-----+\n",
      "|   Melanie Bancilhon|   99|\n",
      "|           Yuming Ba|   99|\n",
      "|      Jana de Wiljes|   99|\n",
      "|          Bert Zwart|   99|\n",
      "|     Rui Pedro Paiva|   99|\n",
      "|  S. M. Kamrul Hasan|   99|\n",
      "|    Stefano Sabatini|   99|\n",
      "|         Kenji Sagae|   99|\n",
      "|   Isabel Beckenbach|   99|\n",
      "|Adriano L. I. Oli...|   99|\n",
      "| Luis Muñoz-González|   99|\n",
      "|       Julia Pritzen|   99|\n",
      "|        Annie Polish|   99|\n",
      "|   Venera Arnaoudova|   99|\n",
      "|         Weikai Miao|   99|\n",
      "|     Du Nguyen Duong|   99|\n",
      "|      Bruno M. Rocha|   99|\n",
      "|          Dan Pelleg|   99|\n",
      "|       Kemele Endris|   99|\n",
      "|  Andrew Lappalainen|   99|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "communities.filter((\"label==99\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}